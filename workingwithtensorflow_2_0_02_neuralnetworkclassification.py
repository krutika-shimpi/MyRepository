# -*- coding: utf-8 -*-
"""WorkingWithTensorflow_2.0:02_NeuralNetworkClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SMg_l5VNGkkMkTAwMvcuWE2En0-qIoCf

## Exercises ðŸ› 

1. Play with neural networks in the [TensorFlow Playground](https://playground.tensorflow.org/) for 10-minutes. Especially try different values of the learning, what happens when you decrease it? What happens when you increase it?
2. Replicate the model pictured in the [TensorFlow Playground diagram](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6,6,6,6,6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true) below using TensorFlow code. Compile it using the Adam optimizer, binary crossentropy loss and accuracy metric. Once it's compiled check a summary of the model.
![tensorflow playground example neural network](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png)
*Try this network out for yourself on the [TensorFlow Playground website](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=6,6,6,6,6&seed=0.51287&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&discretize_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=true&problem_hide=true&noise_hide=true&batchSize_hide=true). Hint: there are 5 hidden layers but the output layer isn't pictured, you'll have to decide what the output layer should be based on the input data.*
3. Create a classification dataset using Scikit-Learn's [`make_moons()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function, visualize it and then build a model to fit it at over 85% accuracy.
4. Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the [classifcation tutorial in the TensorFlow documentation](https://www.tensorflow.org/tutorials/keras/classification) for ideas.
5. Recreate [TensorFlow's](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it.
6. Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after.
7. Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example, plot 3 images of the `T-shirt` class with their predictions.
"""

!wget https://raw.githubusercontent.com/krutika-shimpi/machine_learning_models/main/helper_function.py

"""## Make necessary imports"""

# Make necessary imports
import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split as split
from helper_function import plot_loss_curves
from tensorflow.keras import layers
from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping

"""### Create the toy dataset"""

# Create a classification dataset using Scikit-Learn's make_moons()
X_data, y_data = make_moons(n_samples = 500,
                      noise = 0.05,
                      random_state = 42)

# Let's turn it into a pandas dataframe
clf_data = pd.DataFrame(X_data, columns = [f'X{i + 1}' for i in range(X_data.shape[1])])
clf_data['y'] = y_data
clf_data.head()

"""### Visualize the data"""

# Vizualize the data
sns.scatterplot(x = clf_data['X1'], y = clf_data['X2'], c = y_data, cmap = 'RdYlBu')
plt.title('Classification data')
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()

"""### Split the data into training and testing sets"""

# Split the data into train_test sets
train_data, test_data, train_labels, test_labels = split(X_data,
                                                         y_data,
                                                         test_size = 0.2,
                                                         random_state = 42)
len(train_data), len(train_labels), train_data.shape

"""### Build the Model"""

# Lets build the model to get over 85% accuracy

# Set random seed
tf.random.set_seed(42)

# Build the model
model_1 = tf.keras.Sequential(layers = [
    tf.keras.layers.Input(shape = (train_data.shape[1],)),
    tf.keras.layers.Dense(64, activation = 'relu'),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
    ],
                              name = 'model_1_dense')

# Compile the model
model_1.compile(loss = 'binary_crossentropy',
                optimizer = tf.keras.optimizers.Adam(),
                metrics = ['accuracy'])

# Fit the model
model_1_history = model_1.fit(train_data,
                              train_labels,
                              epochs = 30,
                              validation_data = (test_data, test_labels))

# Plot the curves
plot_loss_curves(model_1_history, classification = True)

"""## Working on a Bigger Dataset

Load the fashion mnist dataset from tensorflow datasets.
"""

# Load the Fashion mnist data
from tensorflow.keras.datasets import fashion_mnist

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Check the shapes of the train and test sets
X_train.shape

X_test.shape

# Let's create the class labels for the dataset
class_names = ["T-shirt/top", "Trouser",	"Pullover",	"Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

"""### Visualize, visualize, visualize

Lets visualize some random images along with the labels.
"""

# Display 12 random images and their labels
plt.figure(figsize = (12, 9))

for i in range(12):
  plt.subplot(4, 3, i+1)
  random_idx = np.random.randint(X_train.shape[0])
  plt.imshow(X_train[random_idx], cmap = plt.cm.binary)
  plt.title(class_names[y_train[random_idx]])
  plt.axis(True)
  plt.subplots_adjust(left=0.1, bottom=0.5, right=0.9, top=0.9, wspace=0.4, hspace=1)

plt.tight_layout()
plt.show()

"""### Lets create a model for multiclass classification"""

y_train.shape

"""### Normalize the images"""

# Scaling or normalizeing the image to bring all the pixel values between 0 and 1
X_train = X_train/255.
X_test = X_test/255.

X_train.min(), X_train.max()

"""### Build the model"""

# Define a custom learning rate schedule function
def lr_schedule(epoch):
    """Learning Rate Schedule

    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.

    # Arguments
        epoch (int): The number of epochs

    # Returns
        lr (float32): learning rate
    """
    lr = 1e-3  # Initial learning rate
    if epoch > 20:
        lr *= 0.5e-3
    elif epoch > 15:
        lr *= 1e-3
    elif epoch > 10:
        lr *= 1e-2
    elif epoch > 5:
        lr *= 1e-1
    return lr

# Set the random seed
tf.random.set_seed(42)

# Build the model
model_2 = tf.keras.Sequential([
   layers.Flatten(input_shape = (28, 28)),
   layers.Dense(units = 128, activation = 'relu'),
   layers.Dense(units = 128, activation = 'relu'),
   layers.Dense(len(class_names), activation = 'softmax')
])

# Compile the model
model_2.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
                optimizer = tf.keras.optimizers.Adam(),
                metrics = ['accuracy'])

# Create a learning rate scheduler callback
lr_scheduler = LearningRateScheduler(lr_schedule)
early_stopping = EarlyStopping(monitor = 'val_loss',
                               patience = 3,
                               restore_best_weights = True)

# Fit the model
model_2_history = model_2.fit(X_train,
                              y_train,
                              epochs = 25,
                              validation_data = (X_test, y_test),
                              validation_steps = int(len(X_test) * 0.10),
                              callbacks = [lr_scheduler, early_stopping])

"""### Evaluate the model"""

# Evaluate the model accuracy on whole test data
model_2_results = model_2.evaluate(X_test, y_test)
model_2_results

# Plot the curves
plot_loss_curves(model_2_history, classification = True)

# Make some predictions
pred_probs = model_2.predict(X_test)
pred_probs

# Convert the probabilities into the labels
y_preds = pred_probs.argmax(axis = 1)
y_preds

"""### Plot random images"""

def plot_random_images(model, images, true_labels, classes):
  """
  It plots the random images with their predicted labels and actual labels.

  Args:
    model: It takes an input model for making predictions.
    images: The data from which we can pick random images
    true_labels: The actual label for the images to compare with.
    classes: To output the respective class for each label.

  Returns:
    "Plots the random images picked from the data along with the actual and predicted labels."
  """
  # Make predictions on the data and convert into its labels
  pred_probs = model.predict(images)
  pred_labels = pred_probs.argmax(axis = 1)

  plt.figure(figsize = (12, 9))
  # Plot 6 random images from the data with their labels
  for i in range(6):
    plt.subplot(2, 3, i + 1)
    random_idx = np.random.randint(len(images)-1)
    target_image = images[random_idx]
    target_label = classes[pred_labels[random_idx]]
    true_label = classes[true_labels[random_idx]]

    # Plot the labels in green if the predictions are correct
    if target_label == true_label:
      color = 'green'
    else:
      color = 'red'

    # Plot the image
    plt.imshow(target_image, extent=[0, 0.4, 0, 0.35], cmap = plt.cm.binary)

    # Adding the xlabel information
    plt.xlabel(f'Actual Label: {true_label}\nPredicted Label: {target_label}\nconfidence: {np.round(tf.reduce_max(pred_probs[random_idx]), 1) * 100}%',
               color = color)
    plt.tight_layout()

# Lets make use of the helper function to plot the random images
plot_random_images(model = model_2,
                   images = X_test,
                   true_labels = y_test,
                   classes = class_names)

# Defining our custom softmax class
def custom_softmax(x):
    """
    Apply the softmax activation function to the input tensor.

    Args:
    x: Input tensor.

    Returns:
    Tensor after applying the softmax function.
    """
    return tf.exp(x) / tf.reduce_sum(tf.exp(x), axis=-1, keepdims=True)

# Define input tensor
input_tensor = tf.constant([1.0, 2.0, 3.0])

# Apply the custom_softmax function
output_tensor = custom_softmax(input_tensor)

# Print the result
print(f'Input tensor: {input_tensor.numpy()}')
print(f'output tensor: {output_tensor.numpy()}')

import itertools
from sklearn.metrics import confusion_matrix

# Our function needs a different name to sklearn's plot_confusion_matrix
def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15):
  """Makes a labelled confusion matrix comparing predictions and ground truth labels.

  If classes is passed, confusion matrix will be labelled, if not, integer class values
  will be used.

  Args:
    y_true: Array of truth labels (must be same shape as y_pred).
    y_pred: Array of predicted labels (must be same shape as y_true).
    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.
    figsize: Size of output figure (default=(10, 10)).
    text_size: Size of output figure text (default=15).

  Returns:
    A labelled confusion matrix plot comparing y_true and y_pred.

  Example usage:
    make_confusion_matrix(y_true=test_labels, # ground truth test labels
                          y_pred=y_preds, # predicted labels
                          classes=class_names, # array of class label names
                          figsize=(15, 15),
                          text_size=10)
  """
  # Create the confustion matrix
  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
  n_classes = cm.shape[0] # find the number of classes we're dealing with

  # Plot the figure and make it pretty
  fig, ax = plt.subplots(figsize=figsize)
  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better
  fig.colorbar(cax)

  # Are there a list of classes?
  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])

  # Label the axes
  ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes), # create enough axis slots for each class
         yticks=np.arange(n_classes),
         xticklabels=labels, # axes will labeled with class names (if they exist) or ints
         yticklabels=labels)

  # Make x-axis labels appear on bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  # Set the threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
             horizontalalignment="center",
             color="white" if cm[i, j] > threshold else "black",
             size=text_size)

# Plot the confusion matrix
make_confusion_matrix(y_true = y_test,
                      y_pred = y_preds,
                      classes = class_names,
                      figsize=(15, 15),
                      text_size=10)

